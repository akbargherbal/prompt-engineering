{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90c79728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b631cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APPENDIX_A_PLACEHOLDER', 'APPENDIX_B_PLACEHOLDER', 'APPENDIX_C_PLACEHOLDER']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "with open('./white_paper.md', encoding='utf-8') as f:\n",
    "    white_paper = f.read()\n",
    "\n",
    "\n",
    "# find placeholders;\n",
    "\n",
    "re.findall('APPENDIX_.*?_PLACEHOLDER', white_paper)\n",
    "\n",
    "list_appendix_placeholders = ['APPENDIX_A_PLACEHOLDER', 'APPENDIX_B_PLACEHOLDER', 'APPENDIX_C_PLACEHOLDER']\n",
    "\n",
    "app_a = Path('./APPENDIX_A')\n",
    "app_b = Path('./APPENDIX_B')\n",
    "app_c = Path('./APPENDIX_C')\n",
    "\n",
    "\n",
    "def fomrat_text_as(path, ext='markdown'):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    output = f'''\n",
    "```{ext}\n",
    "{text}    \n",
    "'''.strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "# concatenate content of each appendix; \n",
    "# ___START_OF_{file_name1}___\n",
    "# content of file 1\n",
    "# ___START_OF_{file_name1}___\n",
    "# ___START_OF_{file_name2}___\n",
    "# content of file 2\n",
    "# ___START_OF_{file_name2}___\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89419e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "with open('./white_paper.md', encoding='utf-8') as f:\n",
    "    white_paper = f.read()\n",
    "\n",
    "\n",
    "# find placeholders;\n",
    "\n",
    "re.findall('APPENDIX_.*?_PLACEHOLDER', white_paper)\n",
    "\n",
    "list_appendix_placeholders = ['APPENDIX_A_PLACEHOLDER', 'APPENDIX_B_PLACEHOLDER', 'APPENDIX_C_PLACEHOLDER']\n",
    "\n",
    "app_a = Path('./APPENDIX_A')\n",
    "app_b = Path('./APPENDIX_B')\n",
    "app_c = Path('./APPENDIX_C')\n",
    "\n",
    "\n",
    "def fomrat_text_as(path, ext='markdown'):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    output = f'''\n",
    "```{ext}\n",
    "{text}    \n",
    "'''.strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "# concatenate content of each appendix; \n",
    "# ___START_OF_{file_name1}___\n",
    "# content of file 1\n",
    "# ___START_OF_{file_name1}___\n",
    "# ___START_OF_{file_name2}___\n",
    "# content of file 2\n",
    "# ___START_OF_{file_name2}___\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54b44bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "White paper updated with appendix content\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "with open('./white_paper.md', encoding='utf-8') as f:\n",
    "    white_paper = f.read()\n",
    "\n",
    "# find placeholders\n",
    "placeholders = re.findall(r'APPENDIX_.*?_PLACEHOLDER', white_paper)\n",
    "\n",
    "list_appendix_placeholders = ['APPENDIX_A_PLACEHOLDER', 'APPENDIX_B_PLACEHOLDER', 'APPENDIX_C_PLACEHOLDER']\n",
    "\n",
    "app_a = Path('./APPENDIX_A')\n",
    "app_b = Path('./APPENDIX_B')\n",
    "app_c = Path('./APPENDIX_C')\n",
    "\n",
    "def format_text_as(path, ext='markdown'):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    output = f'''```{ext}\n",
    "{text}\n",
    "```'''.strip()\n",
    "    return output\n",
    "\n",
    "def concatenate_appendix_content(appendix_path):\n",
    "    \"\"\"\n",
    "    Concatenate content of each file in appendix directory with separators\n",
    "    \"\"\"\n",
    "    content_parts = []\n",
    "    \n",
    "    # Get all files in the appendix directory\n",
    "    for file_path in sorted(appendix_path.iterdir()):\n",
    "        if file_path.is_file():\n",
    "            file_name = file_path.name\n",
    "            \n",
    "            # Add start separator\n",
    "            content_parts.append(f\"___START_OF_{file_name}___\")\n",
    "            \n",
    "            # Add file content\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                file_content = f.read()\n",
    "            content_parts.append(file_content)\n",
    "            \n",
    "            # Add end separator\n",
    "            content_parts.append(f\"___END_OF_{file_name}___\")\n",
    "    \n",
    "    return '\\n'.join(content_parts)\n",
    "\n",
    "# Process each appendix\n",
    "appendix_contents = {}\n",
    "appendix_paths = [app_a, app_b, app_c]\n",
    "\n",
    "for i, appendix_path in enumerate(appendix_paths):\n",
    "    if appendix_path.exists():\n",
    "        placeholder = list_appendix_placeholders[i]\n",
    "        appendix_contents[placeholder] = concatenate_appendix_content(appendix_path)\n",
    "\n",
    "# Replace placeholders in white paper\n",
    "updated_white_paper = white_paper\n",
    "for placeholder, content in appendix_contents.items():\n",
    "    updated_white_paper = updated_white_paper.replace(placeholder, content)\n",
    "\n",
    "# Write the updated white paper\n",
    "with open('./white_paper_with_appendices.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(updated_white_paper)\n",
    "\n",
    "print(\"White paper updated with appendix content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ea00f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in ./SUCCESS_STORIES/SESSION_PROMPT_FACTORY_02_PRK_RECOVERY: dict_keys(['runSettings', 'systemInstruction', 'chunkedPrompt'])\n",
      "Keys in ./SUCCESS_STORIES/SESSION_PROMPT_FACTORY_BLIND_SPOT_NAVIGATOR: dict_keys(['runSettings', 'systemInstruction', 'chunkedPrompt'])\n",
      "Successfully wrote ./success_story_01.json\n",
      "Successfully wrote ./success_story_02.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def read_json_file(filepath):\n",
    "    \"\"\"Read and parse JSON file, return the chunkedPrompt data.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Warning: File {filepath} does not exist\")\n",
    "            return None\n",
    "            \n",
    "        with open(filepath, encoding='utf-8') as f:\n",
    "            json_text = f.read()\n",
    "            json_data = json.loads(json_text)\n",
    "            print(f\"Keys in {filepath}: {json_data.keys()}\")\n",
    "            \n",
    "            if 'chunkedPrompt' not in json_data:\n",
    "                print(f\"Warning: 'chunkedPrompt' key not found in {filepath}\")\n",
    "                return None\n",
    "                \n",
    "            return json_data['chunkedPrompt']\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON in {filepath}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "def write_json_file(data, filepath):\n",
    "    \"\"\"Write data to JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filepath, mode='w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Successfully wrote {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {filepath}: {e}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Read source files\n",
    "    data_01 = read_json_file('./SUCCESS_STORIES/SESSION_PROMPT_FACTORY_02_PRK_RECOVERY')\n",
    "    data_02 = read_json_file('./SUCCESS_STORIES/SESSION_PROMPT_FACTORY_BLIND_SPOT_NAVIGATOR')\n",
    "    \n",
    "    # Write output files if data was successfully read\n",
    "    if data_01 is not None:\n",
    "        write_json_file(data_01, './success_story_01.json')\n",
    "    \n",
    "    if data_02 is not None:\n",
    "        write_json_file(data_02, './success_story_02.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac45237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
