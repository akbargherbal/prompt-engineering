
=== START of worksheet ID: 002 ===
Of course. Here is the complete, user-ready Markdown content for "The Evidence Locker" worksheet, designed to perfectly adhere to your provided style guide.

***

# Step 2: The Evidence Locker

### Gather the Raw Materials for Your Simulation

Welcome to the second phase of your "Mise en Place." In the *Pain Journal*, you articulated the problem in your own words. Now, it’s time to gather the actual evidence. Think of the LLM as a brilliant consultant who has been teleported into your project but is completely blindfolded. They can’t see your screen, your files, or your terminal. The Evidence Locker is where you meticulously collect and label every piece of code, data, and log file they need to "see" your system. By providing this high-fidelity context, you enable the LLM to build an accurate mental model, which is the foundation of any successful simulation.

---

### 2.1 The Code at the Scene

**Why this is important:** The code is the central actor in our drama. For the LLM to understand the system's behavior, it must first understand the logic, structure, and intent of the primary script or functions involved. This isn't just about finding a bug; it's about understanding the entire process the code is trying to execute.

**Your Task:**
Can you paste the most relevant code snippet(s) that are at the heart of the manual process? Include the main function, class, or script that you have to run or interact with. Don't worry about it being perfect; the raw material is what matters.

> _Example: A Python script that iterates through a directory of log files, uses regex to find lines containing 'ERROR', and writes them to a new file._
>
> ```python
> import os
> import re
> 
> def process_log_files(source_dir, output_file):
>     error_pattern = re.compile(r".*ERROR: Task failed for prompt_id: (\w+)")
>     failed_prompts = []
> 
>     with open(output_file, 'w') as out_f:
>         for filename in os.listdir(source_dir):
>             if filename.endswith(".log"):
>                 with open(os.path.join(source_dir, filename), 'r') as log_f:
>                     for line in log_f:
>                         match = error_pattern.search(line)
>                         if match:
>                             prompt_id = match.group(1)
>                             out_f.write(f"{prompt_id}\n")
>                             failed_prompts.append(prompt_id)
>     print(f"Found {len(failed_prompts)} failed prompts.")
> ```

> **(Paste your core code here.)**
>
> ```
>
> ```

---

### 2.2 Inputs and Outputs: The Data's Journey

**Why this is important:** Code doesn't exist in a vacuum; it transforms data. The LLM needs to see the "before" and "after" picture. Providing samples of your input files (the raw material) and the current output files (the flawed result) gives the simulation a clear start and end point. This helps the LLM understand the *shape* of your data and the *nature* of the current failure.

**Your Tasks:**

1.  What does a typical input look like? This could be a few lines from a log file, a sample row from a CSV, or a JSON object from an API call.

    > _Example: A few representative lines from one of the `production_app-2023-10-26.log` files._
    >
    > ```log
    > 2023-10-26 14:22:01,123 INFO: Task started for prompt_id: abc-123
    > 2023-10-26 14:22:02,456 DEBUG: Processing data chunk 1 of 5
    > 2023-10-26 14:22:03,789 ERROR: Task failed for prompt_id: def-456 due to upstream timeout.
    > 2023-10-26 14:22:05,123 INFO: Task completed for prompt_id: ghi-789
    > ```

    > **(Paste your input data sample here.)**
    >
    > ```
    >
    > ```

2.  What does the current, problematic output look like? This could be a manually created file, an incomplete report, or simply an error message printed to the console.

    > _Example: The contents of the `manually_extracted_failures.txt` file I have to create each time._
    >
    > ```text
    > def-456
    > jkl-012
    > mno-345
    > ```

    > **(Paste your current output sample here.)**
    >
    > ```
    >
    > ```

---

### 2.3 The Environment: The Laws of Physics

**Why this is important:** A script that works on your MacBook might fail inside a Docker container. The execution environment defines the "laws of physics" for your code. The LLM needs to know about the operating system, critical dependencies, and any containerization. This prevents it from suggesting a solution that is impossible in your specific context (e.g., using a library you don't have installed).

**Your Tasks:**

1.  How and where is this code executed? Describe the environment.

    > _Example: The script is run manually on my local machine (macOS Sonoma). I have Python 3.11 installed via Homebrew. The log files are pulled from an S3 bucket into a local `./logs` directory before running._

    > **(Describe your execution environment here.)**
    >
    >

2.  What are the key libraries or dependencies? If you have a `requirements.txt`, `package.json`, or `pom.xml`, listing the most relevant packages is perfect.

    > _Example: This is a simple script with no external libraries, just the standard `os` and `re` modules that come with Python._
    >
    > _Alternate Example: Key dependencies from requirements.txt are `pandas==2.1.1`, `boto3==1.28.77`, `requests==2.31.0`._

    > **(List your key dependencies here.)**
    >
    >

---

### 2.4 The Paper Trail: Logs and Error Messages

**Why this is important:** If your code is the actor and the data is the plot, then logs and errors are the narrator, telling you exactly what went wrong and where. These are the most direct clues you can give the LLM. A full stack trace or a verbose error log is a goldmine of information that allows the simulation to pinpoint the root cause of the system's failure.

**Your Task:**
Can you capture the exact output, including any stack traces or error messages, that you see when the process fails or behaves incorrectly?

> _Example: When I accidentally point the script to a malformed log file, it doesn't fail gracefully. It just stops and produces an empty output file with no error message. The *lack* of an error IS the evidence._
>
> _Alternate Example: If a file is locked, I get the following Python stack trace:_
>
> ```traceback
> Traceback (most recent call last):
>   File "process_logs.py", line 18, in <module>
>     process_log_files("./logs", "failures.txt")
>   File "process_logs.py", line 11, in process_log_files
>     with open(os.path.join(source_dir, filename), 'r') as log_f:
> PermissionError: [Errno 13] Permission denied: './logs/production_app-2023-10-26.log'
> ```

> **(Paste your relevant logs and error messages here.)**
>
> ```
>
> ```

***

**Locker Secured.** You have now gathered all the physical evidence. The scene is set, and the key artifacts are tagged and ready for inspection. In the next worksheet, **The Rulebook**, we will define the rules of the game—establishing the constraints, goals, and success criteria for our simulation.
=== END of worksheet ID: 002 ===

=== START of worksheet ID: 006 ===
Here is the complete, user-ready Markdown content for "The Evidence Locker" worksheet.

***

# Step 2: The Evidence Locker

Welcome to the second phase of your "Mise en Place." In the **Pain Journal**, you articulated the *what* and the *why* of your problem. Now, in **The Evidence Locker**, we'll gather the *how*.

For an LLM to act as a world-class systems simulator, it needs the same context a senior developer would demand before tackling a problem. It can't see your screen, access your codebase, or read your logs. We must meticulously collect and present this "evidence." Think of yourself as a detective building a case file. Every artifact you gather here helps the LLM build a high-fidelity mental model of your system, which is the absolute key to a successful simulation.

---

### 2.1 The Code: Your System's DNA

First, let's gather the code. The code contains the explicit logic, the functions, and the "rules of physics" for your specific system. Providing the right snippets is the most direct way to show the LLM *how* your application is designed to behave. We don't need the whole application, just the pieces directly related to the pain point.

**What are the most relevant code snippets (functions, classes, methods) that are directly involved in the process you're trying to simulate or fix?**
*Focus on the code that 'touches' the pain point you identified in your Pain Journal.*

> _Example: The `process_log_file(file_path)` function that opens a file, reads it line by line, and attempts to parse a JSON object from each line. It also contains the `try...except` block where the error occurs._
>
> ```python
> def process_log_file(file_path):
>     with open(file_path, 'r') as f:
>         for i, line in enumerate(f):
>             try:
>                 data = json.loads(line)
>                 # ... further processing ...
>             except json.JSONDecodeError as e:
>                 print(f"Error on line {i}: {e}")
>                 # ... error handling logic ...
> ```
>
> **[Paste your relevant code snippets here. Use Markdown for formatting.]**

**How is this code invoked? What are the key function signatures, API endpoints, or command-line arguments that trigger this process?**
*This helps the simulator understand the entry point and the initial state of the system.*

> _Example: The script is run from the command line like this: `python process_logs.py --source s3://my-log-bucket/2023-10-26/`. The `main()` function uses `argparse` to handle the arguments and then calls `process_log_file()` for each file found._
>
> **[Describe the entry point and invocation method here.]**

---

### 2.2 The Data & Logs: Eyewitness Testimony

If your code is the system's DNA, then your data and logs are the eyewitness accounts. They show what *actually happened* when the code was executed. This includes the inputs that caused the problem and the outputs (or errors) that resulted. This is often the most critical evidence you can provide.

**What does a small, representative sample of the INPUT data look like?**
*If it's structured (like JSON or CSV), include the schema or headers if possible.*

> _Example: A single valid line from an input log file might be `{"timestamp": "2023-10-26T10:00:05Z", "level": "INFO", "user_id": "abc-123", "event": "login_success"}`. A line that causes an error might be truncated or have malformed JSON, like `{"timestamp": "2023-10-26T10:00:06Z", "level": "ERROR",`._
>
> **[Paste a sample of your input data here.]**

**What do the logs or error messages look like when the process fails?**
*Provide the full traceback if available. This is a roadmap that points directly to the scene of the crime.*

> _Example: The console output shows a clear decoding error and traceback._
>
> ```
> ERROR:root:Failed to process line 1138 in file_x.log.
> Traceback (most recent call last):
>   File "process_logs.py", line 42, in process_log_file
>     data = json.loads(line)
>   File "/usr/lib/python3.9/json/__init__.py", line 346, in loads
>     return _default_decoder.decode(s)
>   File "/usr/lib/python3.9/json/decoder.py", line 337, in decode
>     obj, end = self.raw_decode(s, idx=_w(s, 0).end())
> json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
> ```
>
> **[Paste your error logs and tracebacks here.]**

**Can you provide a clear example of a "good" input that processes correctly AND a "bad" input that causes the failure?**
*This contrast is extremely valuable for the LLM. It allows it to pattern-match the difference between success and failure.*

> _Example:_
> *   _**Good Input:** `{"id": 1, "status": "complete", "payload": {"value": 100}}`_
> *   _**Bad Input:** `{"id": 2, "status": "` (truncated file causes invalid JSON)_
>
> **[Provide your 'good' vs. 'bad' input examples here.]**

---

### 2.3 The Environment: The Scene of the Crime

Code doesn't run in a vacuum. The environment—its libraries, OS, cloud services, and configurations—can fundamentally change behavior. This information helps the LLM understand external factors and constraints that aren't visible in the code itself.

**What is the execution environment?**
*Include the programming language version, key libraries and their versions, and the operating system or container base image.*

> _Example: Python 3.9.12, running in a Docker container based on `python:3.9-slim-buster`. Key libraries: `boto3==1.26.0`, `pandas==1.5.3`._
>
> **[Describe your runtime environment and key dependencies here.]**

**What is the high-level system architecture?**
*Where does this code run, and what external services does it interact with? (e.g., AWS Lambda, a specific database, a third-party API, a local file system).*

> _Example: The Python script is deployed as an AWS Lambda function. It's triggered by an S3 "Object Created" event. It reads the new file from the trigger bucket (`raw-logs`) and, on success, writes a summary file to a different bucket (`processed-summaries`)._
>
> **[Describe the system architecture and external interactions here.]**

**Are there any important configuration settings, environment variables, or resource limits?**
*Think about things like timeouts, memory limits, API keys (names only, not secrets!), or feature flags that could affect the process.*

> _Example: The Lambda function has a 128MB memory limit and a 30-second execution timeout. It uses an environment variable `TARGET_BUCKET=processed-summaries`._
>
> **[List relevant configuration, environment variables, or resource limits here.]**
=== END of worksheet ID: 006 ===

=== START of worksheet ID: 010 ===
Here is the complete, user-ready Markdown content for "The Evidence Locker" worksheet.

---

# Step 2: The Evidence Locker

### **Gathering the Ground Truth for Your Simulation**

Welcome to the Evidence Locker. Think of yourself as a detective building a case file. An LLM, like any great detective, can't solve a mystery without evidence. It needs to see the "scene of the crime" (your code), read the "witness statements" (your logs), and understand the "background checks" (your documentation). This step is where you gather all the concrete artifacts that define the reality of your system. By providing this "ground truth," you empower the LLM to build a high-fidelity mental model of your problem, ensuring its proposed solutions are relevant and practical, not just theoretical.

---

### **Section 1: The Code Base (The System's DNA)**

First, we need the code. This is the absolute ground truth of how your system currently behaves. The LLM needs to analyze the exact logic—the functions, the variables, the control flow—to understand the mechanics of the problem you're trying to solve. Without it, the LLM is just guessing about your implementation.

**1. What are the key scripts, modules, or functions at the heart of the manual process you documented in your Pain Journal?**
Don't worry about providing every single line of code yet. Just identify the most relevant files or code blocks.

> _Example: The main Python script is `batch_processor.py`, which ingests user data. It heavily uses the `api_client` module in `utils/helpers.py` to make external calls. The whole process is kicked off by `run_nightly_job.sh`._
>
> **Your Answer:**
>
> >

**2. Are there any important configuration files (`.yml`, `.json`, `.env`, etc.) that control the behavior of this code?**
Think about files that define timeouts, feature flags, API endpoints, or other environmental parameters.

> _Example: The `config.yml` file defines API endpoints and retry logic. The `.env` file contains the `API_TIMEOUT_SECONDS=30` variable, which is a frequent source of our problems._
>
> **Your Answer:**
>
> >

**3. Where can these files be found? List file paths, Git repo URLs, or simply paste the most relevant snippets.**
The goal is to have the actual content ready to be included in your final prompt.

> _Example: All files are in the `data-processing-tools` repo under the `src/nightly_job` directory. The most critical function is `process_record()` in `batch_processor.py`._
>
> **Your Answer:**
>
> >

---

### **Section 2: The Digital Trail (Logs, Errors, and Outputs)**

If code is the system's DNA, then logs and outputs are the story of that DNA in action. They are the objective record of what *actually happened*—the errors, the successes, and the unexpected behaviors. This digital trail provides the LLM with the critical symptoms of the problem, moving from theory to tangible reality.

**1. What does a "good" log entry or a successful output look like for this process?**
This helps the LLM establish a baseline for normal, correct behavior.

> _Example: A successful log line looks like this: `[2023-10-26 02:30:15] INFO: Batch 123 completed successfully. 500 records processed in 12.5s.`_
>
> **Your Answer:**
>
> >

**2. Crucially, what do the "bad" logs, error messages, or failure outputs look like?**
This is the core evidence of the pain you documented. Be as specific as possible. Paste entire stack traces if you have them.

> _Example: We see two main errors. One is a Python traceback ending in `KeyError: 'user_id' not found in record`. The other is a custom log message: `[2023-10-26 03:15:45] ERROR: API timeout after 30 seconds for job_id 456.`_
>
> **Your Answer:**
>
> >

**3. Are there any other system outputs? (e.g., generated files, database entries, dashboard screenshots)**
Think about any artifacts that are created, modified, or left behind by the process. This shows the LLM the consequences of the system's execution.

> _Example: After a failure, I have to manually create a `failed_prompts.csv` file that contains the IDs of the records that didn't process, so I can re-run them later._
>
> **Your Answer:**
>
> >

---

### **Section 3: The Unwritten Rules (Documentation & Tribal Knowledge)**

Not all critical information lives in code or logs. Some of it exists in documentation, and even more exists only in the heads of the people who run the system. This "tribal knowledge" contains the hidden assumptions and the operational wisdom that the LLM can't possibly infer on its own.

**1. Is there any formal documentation (e.g., READMEs, Confluence/Notion pages, API docs) that describes how this system is _supposed_ to work?**
Even if it's out of date, it provides context on the system's original intent. Provide links or summaries.

> _Example: The `README.md` in the repo explains the initial setup, but it's six months out of date. The external API we call has official documentation at `api.vendor.com/docs`._
>
> **Your Answer:**
>
> >

**2. What is the "tribal knowledge" about this process? What's the critical information you'd have to tell a new team member that isn't written down anywhere?**
This is one of the most valuable inputs you can provide. Think about operational quirks, secret workarounds, or known issues.

> _Example: "Oh, you have to make sure you run the `cleanup_temp_files.sh` script first, otherwise the nightly job might pick up zombie files from a previous failed run. Also, the vendor API gets really slow between 2-3 AM EST, so we try to avoid running big jobs then, even though that's our main processing window."_
>
> **Your Answer:**
>
> >

***

**Worksheet Complete.** You have now assembled a comprehensive evidence file. With this collection of code, logs, and context, the LLM won't be operating in a vacuum. It will be reasoning about your system based on the same evidence you have. You're ready for the next step: **The Rulebook**, where we'll define the constraints and success criteria for a solution.
=== END of worksheet ID: 010 ===
